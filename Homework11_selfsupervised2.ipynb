{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework11_selfsupervised2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eugenemfu/DUL_HW/blob/main/Homework11_selfsupervised2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!if [ -d dul_2021 ]; then rm -Rf dul_2021; fi\n",
        "!git clone https://github.com/GrigoryBartosh/dul_2021\n",
        "!pip install ./dul_2021"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dRz5SIFKfZD",
        "outputId": "33bb2fb5-9d07-43e1-c05d-f959d9ec556d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dul_2021'...\n",
            "remote: Enumerating objects: 380, done.\u001b[K\n",
            "remote: Counting objects: 100% (217/217), done.\u001b[K\n",
            "remote: Compressing objects: 100% (144/144), done.\u001b[K\n",
            "remote: Total 380 (delta 122), reused 96 (delta 66), pack-reused 163\u001b[K\n",
            "Receiving objects: 100% (380/380), 55.90 MiB | 21.80 MiB/s, done.\n",
            "Resolving deltas: 100% (179/179), done.\n",
            "Processing ./dul_2021\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Building wheels for collected packages: dul-2021\n",
            "  Building wheel for dul-2021 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dul-2021: filename=dul_2021-0.1.0-py3-none-any.whl size=27628 sha256=cef8a1dd6625979e59d950589ace083eaa966bea1d11ee496103a9bf76952da4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0tenyacv/wheels/55/59/29/0fb1c635652157734f4d741f32fc11979149684e83e919de06\n",
            "Successfully built dul-2021\n",
            "Installing collected packages: dul-2021\n",
            "Successfully installed dul-2021-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dul_2021.utils.hw11_utils import *"
      ],
      "metadata": {
        "id": "KjY-iIy5MSZb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1. BYOL\n",
        "\n",
        "Here we will implement [BYOL](https://arxiv.org/abs/2006.07733).\n",
        "\n",
        "* You can combine view, representation, and projection into one network. You can use same architechure as in practice. \n",
        "\n",
        "* Use BatchNorm\n",
        "\n",
        "* As predictor use few linear layers\n",
        "\n",
        "* Dataset comes untransformed, so you need to apply transformations during training by yourself. Use same augmentations as in SimCLR\n",
        "\n",
        "**Hyperparameters**\n",
        "\n",
        "* Ï„ = 0.99 (target update coefficient)\n",
        "* lr = 1e-4\n",
        "* num_epochs = 20\n",
        "* latent dim = 128\n",
        "\n",
        "\n",
        "\n",
        "**You will provide the following deliverables**\n",
        "\n",
        "\n",
        "1. Over the course of training, record loss ber batch.\n",
        "2. A function that encodes a batch of images with your trained model. The function recieves a batch torch tensors on cpu and should return transformed 2d tensor (batch size x laten dim). It will be used to test representation on classification task."
      ],
      "metadata": {
        "id": "t7J5FgOHW6Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = 'cuda'"
      ],
      "metadata": {
        "id": "FbnyhCeh7J6i"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, out_dim=128, hid_dim_full=128):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(16, 16, 3, padding=1, stride=2)\n",
        "        self.conv3 = nn.Conv2d(16, 32, 5, padding=2)\n",
        "        self.conv4 = nn.Conv2d(32, 32, 3, padding=1, stride=2)\n",
        "        self.conv5 = nn.Conv2d(32, 32, 1)\n",
        "        self.conv6 = nn.Conv2d(32, 4, 1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        self.bn4 = nn.BatchNorm2d(32)\n",
        "        self.bn5 = nn.BatchNorm2d(32)\n",
        "        self.bn6 = nn.BatchNorm2d(4)\n",
        "        self.conv_to_fc = 7 * 7 * 4\n",
        "        self.fc1 = nn.Linear(self.conv_to_fc, hid_dim_full)\n",
        "        self.fc2 = nn.Linear(hid_dim_full, int(hid_dim_full // 2))\n",
        "        self.features = nn.Linear(int(hid_dim_full // 2), out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = F.relu(self.bn6(self.conv6(x)))\n",
        "        x = x.view(-1, self.conv_to_fc)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        features = self.features(x)\n",
        "        return features\n",
        "\n",
        "\n",
        "class BYOL_dataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomResizedCrop(28),\n",
        "            transforms.Normalize(0.5, 0.5),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return [self.transforms(self.dataset[i][0]) for _ in range(2)]\n",
        "\n",
        "\n",
        "class BYOL(nn.Module):\n",
        "    def __init__(self, hidden_dim=256, latent_dim=128, num_epochs=20, lr=1e-4, tau=0.99):\n",
        "        super().__init__()\n",
        "        self.teacher = Net()\n",
        "        self.student = copy.deepcopy(self.teacher)\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim), \n",
        "            nn.BatchNorm1d(hidden_dim), \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim), \n",
        "            nn.BatchNorm1d(hidden_dim), \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, latent_dim),\n",
        "        )\n",
        "        self.teacher.requires_grad_(False)\n",
        "        self.num_epochs = num_epochs\n",
        "        self.lr = lr\n",
        "        self.tau = tau\n",
        "\n",
        "    def update(self):\n",
        "        for t, s in zip(self.teacher.parameters(), self.student.parameters()):\n",
        "            t.data = self.tau * t.data + (1 - self.tau) * s.data\n",
        "\n",
        "    def similarity(self, x, y):\n",
        "        x_ = self.predictor(self.student(x))\n",
        "        with torch.no_grad():\n",
        "            y_ = self.teacher(y).detach()\n",
        "        cos = torch.sum(x_ * y_, dim=1) / (torch.norm(x_, dim=1) * torch.norm(y_, dim=1))\n",
        "        return 2 * (1 - cos)\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        return (self.similarity(x, y) + self.similarity(y, x)).mean()\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = x.to(device)\n",
        "        self.student.eval()\n",
        "        out = self.student(x)\n",
        "        self.student.train()\n",
        "        return out\n",
        "\n",
        "    def fit(self, train_data):\n",
        "        opt = Adam(self.parameters(), lr=self.lr)\n",
        "        dataset = BYOL_dataset(train_data)\n",
        "        dataloader = DataLoader(dataset, batch_size=256)\n",
        "        losses = []\n",
        "        for epoch in tqdm(range(self.num_epochs)):\n",
        "            for x, y in dataloader:\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "                loss = self.loss(x, y)\n",
        "                opt.zero_grad()\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "                self.update()\n",
        "                losses.append(loss.item())\n",
        "        return np.array(losses)"
      ],
      "metadata": {
        "id": "w546Q6GyvXZU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q1(train_data):\n",
        "    \"\"\"\n",
        "    train_data: An (n_train, 1, 28, 28) torchvision dataset of MNIST images with values from -1 to 1\n",
        "\n",
        "    Returns\n",
        "    - a (# of training iterations, ) numpy array  losses on each iteration\n",
        "    - a function that transforms batch of images into their latent representation\n",
        "    \"\"\"\n",
        "    byol = BYOL(num_epochs=20).to(device)\n",
        "    losses = byol.fit(train_data)\n",
        "    return losses, byol.encode"
      ],
      "metadata": {
        "id": "Og9Fv7sV6nrO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change flag to False if you want only to test your losses w/o accuracy (it takes around 4-5 minutes)\n",
        "q1_results(q1, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "3Ib0ne2mX80s",
        "outputId": "7eac7ccb-d5e1-4fbf-f6ad-963f59245801"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [27:54<00:00, 83.71s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean classification accuracy=0.5553\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2. Barlow Twins\n",
        "\n",
        "Here we will implement [barlow twins](https://arxiv.org/abs/2103.03230).\n",
        "\n",
        "* You can use same architechure as in practice. \n",
        "\n",
        "* Dataset comes untransformed, so you need to apply transformations during training by yourself. Use same augmentations as in SimCLR\n",
        "\n",
        "**Hyperparameters**\n",
        "\n",
        "* Î» = 0.01 \n",
        "* lr = 5e-4\n",
        "* num_epochs = 20\n",
        "* latent dim = 128\n",
        "\n",
        "\n",
        "\n",
        "**You will provide the following deliverables**\n",
        "\n",
        "\n",
        "1. Over the course of training, record loss ber batch.\n",
        "2. A function that encodes a batch of images with your trained model. The function recieves a batch torch tensors on cpu and should return transformed 2d tensor (batch size x laten dim). It will be used to test representation on classification task."
      ],
      "metadata": {
        "id": "hlbaIthyMGab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q2(train_data):\n",
        "    \"\"\"\n",
        "    train_data: An (n_train, 1, 32, 32) torchvision dataset of CIFAR10 images with values from -1 to 1\n",
        "\n",
        "    Returns\n",
        "    - a (# of training iterations, ) numpy array  losses on each iteration\n",
        "    - a function that transforms batch of images into their latent representation\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "pd6RrZfP75HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change flag to False if you want only to test your losses w/o accuracy (it takes around 4-5 minutes)\n",
        "q2_results(q2, True)"
      ],
      "metadata": {
        "id": "EA1Z_s1a8_sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus. SwAV\n",
        "\n",
        "Here we will implement [SwAV](https://arxiv.org/abs/2006.09882v5)\n",
        "\n",
        "**You will provide the following deliverables**\n",
        "\n",
        "\n",
        "1. Over the course of training, record loss ber batch.\n",
        "2. A function that encodes a batch of images with your trained model. The function recieves a batch torch tensors on cpu and should return transformed 2d tensor (batch size x laten dim). It will be used to test representation on classification task."
      ],
      "metadata": {
        "id": "D8UN9nr9aYGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def b(train_data):\n",
        "    \"\"\"\n",
        "    train_data: An (n_train, 1, 32, 32) torchvision dataset of CIFAR10 images with values from -1 to 1\n",
        "\n",
        "    Returns\n",
        "    - a (# of training iterations, ) numpy array  losses on each iteration\n",
        "    - a function that transforms batch of images into their latent representation\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "1Yin_8Ebaa8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q2_results(b, True)"
      ],
      "metadata": {
        "id": "S0jgIgLN8tzd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}